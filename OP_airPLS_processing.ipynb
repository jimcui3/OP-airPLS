{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2cd33409",
   "metadata": {},
   "source": [
    "#### This code is intended for utilization by OP-airPLS in the context of executing batch processing operations with actual datasets.\n",
    "\n",
    "It is permissible to manually adjust several parameters, contingent upon the user's familiarity with the optimized parameters. These adjustments include lambda, tau, p-order (set to 2 by default), max iteration (default: 1000), and an option to determine whether airPLS operates on a normalized spectrum (in which case it is subsequently unnormalized) or not.\n",
    "\n",
    "Inquiries regarding the utilization of this code are welcomed. Please contact: jiaheng.cui@uga.edu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6bc1e306-54d3-4b0b-9859-cf56ff8e03c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import trange\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import savgol_filter\n",
    "from scipy.stats import norm\n",
    "from scipy.sparse import csc_matrix, eye, diags\n",
    "from scipy.sparse.linalg import spsolve\n",
    "from scipy.optimize import minimize\n",
    "import warnings\n",
    "from scipy import integrate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be6d33d1",
   "metadata": {},
   "source": [
    "This code is modified from Yizeng Liang and Zhang Zhimin (https://github.com/zmzhang/airPLS). Their liscence is preserved according to their requirement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e36b02f4-78b4-4a3b-81d9-45b70428f26c",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "airPLS.py Copyright 2014 Renato Lombardo - renato.lombardo@unipa.it\n",
    "Baseline correction using adaptive iteratively reweighted penalized least squares\n",
    "\n",
    "This program is a translation in python of the R source code of airPLS version 2.0\n",
    "by Yizeng Liang and Zhang Zhimin - https://code.google.com/p/airpls\n",
    "Reference:\n",
    "Z.-M. Zhang, S. Chen, and Y.-Z. Liang, Baseline correction using adaptive iteratively reweighted penalized least squares. Analyst 135 (5), 1138-1146 (2010).\n",
    "\n",
    "Description from the original documentation:\n",
    "\n",
    "Baseline drift always blurs or even swamps signals and deteriorates analytical results, particularly in multivariate analysis.  It is necessary to correct baseline drift to perform further data analysis. Simple or modified polynomial fitting has been found to be effective in some extent. However, this method requires user intervention and prone to variability especially in low signal-to-noise ratio environments. The proposed adaptive iteratively reweighted Penalized Least Squares (airPLS) algorithm doesn't require any user intervention and prior information, such as detected peaks. It iteratively changes weights of sum squares errors (SSE) between the fitted baseline and original signals, and the weights of SSE are obtained adaptively using between previously fitted baseline and original signals. This baseline estimator is general, fast and flexible in fitting baseline.\n",
    "\n",
    "\n",
    "LICENCE\n",
    "This program is free software: you can redistribute it and/or modify\n",
    "it under the terms of the GNU Lesser General Public License as published by\n",
    "the Free Software Foundation, either version 3 of the License, or\n",
    "(at your option) any later version.\n",
    "\n",
    "This program is distributed in the hope that it will be useful,\n",
    "but WITHOUT ANY WARRANTY; without even the implied warranty of\n",
    "MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n",
    "GNU Lesser General Public License for more details.\n",
    "\n",
    "You should have received a copy of the GNU Lesser General Public License\n",
    "along with this program.  If not, see <http://www.gnu.org/licenses/>\n",
    "'''\n",
    "\n",
    "def WhittakerSmooth(x, w, lambda_, differences=1):\n",
    "    # print(\"\\n--- Starting WhittakerSmooth ---\")\n",
    "    X = np.matrix(x)\n",
    "    m = X.size\n",
    "    E = eye(m, format='csc')\n",
    "    for i in range(differences):\n",
    "        E = E[1:] - E[:-1]\n",
    "    W = diags(w, 0, shape=(m,m))\n",
    "    \n",
    "    A = csc_matrix(W + (lambda_ * E.T * E))\n",
    "    B = csc_matrix(W * X.T)\n",
    "    \n",
    "    try:\n",
    "        background = spsolve(A, B)\n",
    "    except Exception as e:\n",
    "        print(f\"Error in spsolve: {str(e)}\")\n",
    "        return None\n",
    "        \n",
    "    # print(\"--- Finished WhittakerSmooth ---\")\n",
    "    return np.array(background), W, A, B\n",
    "\n",
    "def airPLS(x, lambda_=100, tau=0.001, porder=2, itermax=1000, normalization=False):\n",
    "    # print(f\"\\n--- Starting airPLS with lambda={lambda_}, tau={tau}, itermax={itermax} ---\")\n",
    "    try:\n",
    "        m = x.shape[0]\n",
    "        if normalization:\n",
    "            # print(\"Using normalization\")\n",
    "            original_spectrum = x.copy()\n",
    "            # Try auto area normalization\n",
    "            wavenumber = np.arange(m) # Fake wavenumber since we don't know the true wavenumber, but we just want a normalizing constant\n",
    "            # Check for NaN or inf values\n",
    "            if np.any(np.isnan(x)) or np.any(np.isnan(wavenumber)) or np.any(np.isinf(x)) or np.any(np.isinf(wavenumber)):\n",
    "                print(\"Warning: NaN or inf values detected in the data\")\n",
    "            else:\n",
    "                # Calculate the area using trapezoidal rule\n",
    "                scaling_factor = integrate.trapz(x, wavenumber)\n",
    "                if scaling_factor == 0:\n",
    "                    scaling_factor == 1\n",
    "                    print(\"Warning: Area is zero, using original spectrum\")\n",
    "                # print(scaling_factor)\n",
    "                \n",
    "                # Normalize the spectrum\n",
    "                normalized_x = x / scaling_factor\n",
    "        \n",
    "        weights = np.ones(m)\n",
    "        # Store iteration data\n",
    "        z_iterations = []\n",
    "        d_iterations = []\n",
    "        dssn_iterations = []\n",
    "        weights_iterations = []\n",
    "\n",
    "        for i in range(1, itermax+1):\n",
    "            # print(f\"Working on iteration={i}\")\n",
    "            if normalization:\n",
    "                z, W, A, B = WhittakerSmooth(normalized_x, weights, lambda_, porder)\n",
    "            else:\n",
    "                z, W, A, B = WhittakerSmooth(x, weights, lambda_, porder)\n",
    "            if z is None:\n",
    "                print(f\"WhittakerSmooth failed at iteration {i}\")\n",
    "                return None\n",
    "            \n",
    "            if normalization:\n",
    "                d = normalized_x - z\n",
    "            else:\n",
    "                d = x - z\n",
    "            dssn = np.abs(d[d<0].sum())\n",
    "\n",
    "            # Store iteration data\n",
    "            z_iterations.append(z)\n",
    "            d_iterations.append(d)\n",
    "            dssn_iterations.append(dssn)\n",
    "            weights_iterations.append(weights.copy())\n",
    "            if normalization:\n",
    "                if(dssn < tau * (abs(normalized_x)).sum() or i == itermax):\n",
    "                    # print(f\"Stopping criterion met at iteration {i}, dssn={dssn}, tau * (abs(normalized_x)).sum()={tau * (abs(normalized_x)).sum()}\")\n",
    "                    break\n",
    "            else:\n",
    "                if(dssn < tau * (abs(x)).sum() or i == itermax):\n",
    "                    # print(f\"Stopping criterion met at iteration {i}, dssn={dssn}, tau * (abs(x)).sum()={tau * (abs(x)).sum()}\")\n",
    "                    break\n",
    "            weights[d>=0] = 0\n",
    "\n",
    "            try:\n",
    "                with warnings.catch_warnings(record=True) as caught_warnings:\n",
    "                    warnings.simplefilter(\"always\")\n",
    "                    if np.isinf(dssn):\n",
    "                        dssn = np.finfo(dssn.dtype).max\n",
    "                    exp_term = i * np.abs(d[d<0]) / dssn\n",
    "                    max_exp = 709  # round(np.log(np.finfo(float).max))\n",
    "                    exp_term = np.minimum(exp_term, max_exp)\n",
    "                    weights[d<0] = np.exp(exp_term)\n",
    "                    if caught_warnings:\n",
    "                        warning = caught_warnings[0]\n",
    "                        print(f\"Warning at iteration {i} in exp calculation: {warning.message}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Exception at iteration {i} in exp calculation: {e}\")\n",
    "                if normalization:\n",
    "                    normalized_original_spectrum = original_spectrum / scaling_factor\n",
    "                    normalized_predicted_x = normalized_original_spectrum - z\n",
    "                    unnormalized_predicted_x = normalized_predicted_x * scaling_factor\n",
    "                    z = original_spectrum - unnormalized_predicted_x\n",
    "                return z\n",
    "\n",
    "            try:\n",
    "                if np.isinf(dssn):\n",
    "                    dssn = np.finfo(dssn.dtype).max\n",
    "                weights[0] = np.exp(i * (d[d<0]).max() / dssn)\n",
    "            except Warning as warn:\n",
    "                print(f\"Warning at iteration {i} in w[0] calculation: {warn}\")\n",
    "                if normalization:\n",
    "                    normalized_original_spectrum = original_spectrum / scaling_factor\n",
    "                    normalized_predicted_x = normalized_original_spectrum - z\n",
    "                    unnormalized_predicted_x = normalized_predicted_x * scaling_factor\n",
    "                    z = original_spectrum - unnormalized_predicted_x\n",
    "                return z\n",
    "\n",
    "            weights[-1] = weights[0]\n",
    "\n",
    "        if normalization:\n",
    "            normalized_original_spectrum = original_spectrum / scaling_factor\n",
    "            normalized_predicted_x = normalized_original_spectrum - z\n",
    "            unnormalized_predicted_x = normalized_predicted_x * scaling_factor\n",
    "            z = original_spectrum - unnormalized_predicted_x\n",
    "        return z\n",
    "    except Exception as e:\n",
    "        print(f\"Error in airPLS: {str(e)}\")\n",
    "        print(f\"Traceback: {traceback.format_exc()}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84dfba11",
   "metadata": {},
   "source": [
    "#### Example usage: Process all csvs under the root folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "879d847f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the folder containing the CSV files (replace with your actual path)\n",
    "folder_path = r'E:\\Jiaheng Cui\\Baseline removal\\LPS SERS spectra\\Interpolated CSV-2'\n",
    "\n",
    "# Process each CSV file in the folder\n",
    "for file_name in os.listdir(folder_path):\n",
    "    if file_name.endswith('.csv'):  # Check if the file is a CSV\n",
    "        print(\"Processing \" + file_name)\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        df = pd.read_csv(file_path, sep='\\t')\n",
    "        \n",
    "        # Apply airPLS to every column except the first one\n",
    "        for col in df.columns[1:]:\n",
    "            baseline = airPLS(df[col].values, lambda_=1e4, tau=1e-5, porder=2, itermax=1000, normalization=True)\n",
    "            df[col] = df[col].values - baseline\n",
    "        \n",
    "        # Define the new file name\n",
    "        new_file_name = file_name.replace('.csv', '_airPLS_corrected.csv')\n",
    "        new_file_path = os.path.join(folder_path, new_file_name)\n",
    "        \n",
    "        # Save the processed dataframe to a new CSV file\n",
    "        df.to_csv(new_file_path, sep='\\t', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
