{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6204bce5",
   "metadata": {},
   "source": [
    "#### This code is intended for utilization by OP-airPLS in the context of batch-process the optimization operations with simluated datasets.\n",
    "\n",
    "Inquiries regarding the utilization of this code are welcomed. Please contact: jiaheng.cui@uga.edu."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1176f2d1",
   "metadata": {},
   "source": [
    "# Grid search and optimization for OP-airPLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d28eecf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.sparse import csc_matrix, eye, diags\n",
    "from scipy.sparse.linalg import spsolve\n",
    "from scipy.optimize import minimize\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "\n",
    "\n",
    "# This code is modified from Yizeng Liang and Zhang Zhimin (https://github.com/zmzhang/airPLS). Their liscence is preserved according to their requirement.\n",
    "'''\n",
    "airPLS.py Copyright 2014 Renato Lombardo - renato.lombardo@unipa.it\n",
    "Baseline correction using adaptive iteratively reweighted penalized least squares\n",
    "\n",
    "This program is a translation in python of the R source code of airPLS version 2.0\n",
    "by Yizeng Liang and Zhang Zhimin - https://code.google.com/p/airpls\n",
    "Reference:\n",
    "Z.-M. Zhang, S. Chen, and Y.-Z. Liang, Baseline correction using adaptive iteratively reweighted penalized least squares. Analyst 135 (5), 1138-1146 (2010).\n",
    "\n",
    "Description from the original documentation:\n",
    "\n",
    "Baseline drift always blurs or even swamps signals and deteriorates analytical results, particularly in multivariate analysis.  It is necessary to correct baseline drift to perform further data analysis. Simple or modified polynomial fitting has been found to be effective in some extent. However, this method requires user intervention and prone to variability especially in low signal-to-noise ratio environments. The proposed adaptive iteratively reweighted Penalized Least Squares (airPLS) algorithm doesn't require any user intervention and prior information, such as detected peaks. It iteratively changes weights of sum squares errors (SSE) between the fitted baseline and original signals, and the weights of SSE are obtained adaptively using between previously fitted baseline and original signals. This baseline estimator is general, fast and flexible in fitting baseline.\n",
    "\n",
    "\n",
    "LICENCE\n",
    "This program is free software: you can redistribute it and/or modify\n",
    "it under the terms of the GNU Lesser General Public License as published by\n",
    "the Free Software Foundation, either version 3 of the License, or\n",
    "(at your option) any later version.\n",
    "\n",
    "This program is distributed in the hope that it will be useful,\n",
    "but WITHOUT ANY WARRANTY; without even the implied warranty of\n",
    "MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n",
    "GNU Lesser General Public License for more details.\n",
    "\n",
    "You should have received a copy of the GNU Lesser General Public License\n",
    "along with this program.  If not, see <http://www.gnu.org/licenses/>\n",
    "'''\n",
    "\n",
    "def WhittakerSmooth(x, w, lambda_, differences=1):\n",
    "    X = np.matrix(x)\n",
    "    m = X.size\n",
    "    E = eye(m, format='csc')\n",
    "    for i in range(differences):\n",
    "        E = E[1:] - E[:-1]\n",
    "    W = diags(w, 0, shape=(m,m))\n",
    "    A = csc_matrix(W + (lambda_ * E.T * E))\n",
    "    B = csc_matrix(W * X.T)\n",
    "    background = spsolve(A, B)\n",
    "    return np.array(background)\n",
    "\n",
    "\n",
    "def airPLS(x, lambda_=100, porder=1, itermax=15, tol=0.001):\n",
    "#     print(f\"\\n--- Starting airPLS with lambda={lambda_}, tol={tol}, itermax={itermax} ---\")\n",
    "    \n",
    "    try:\n",
    "        m = x.shape[0]\n",
    "        weights = np.ones(m)\n",
    "\n",
    "        # Store iteration data\n",
    "        z_iterations = []\n",
    "        d_iterations = []\n",
    "        dssn_iterations = []\n",
    "        weights_iterations = []\n",
    "\n",
    "        for i in range(1, itermax+1):\n",
    "            z = WhittakerSmooth(x, weights, lambda_, porder)\n",
    "            if z is None:\n",
    "                print(f\"WhittakerSmooth failed at iteration {i}\")\n",
    "                return None\n",
    "                  \n",
    "            d = x - z\n",
    "            dssn = np.abs(d[d<0].sum())\n",
    "            \n",
    "            # Store iteration data\n",
    "            z_iterations.append(z)\n",
    "            d_iterations.append(d)\n",
    "            dssn_iterations.append(dssn)\n",
    "            weights_iterations.append(weights.copy())\n",
    "\n",
    "            if(dssn < tol * (abs(x)).sum() or i == itermax):\n",
    "                break\n",
    "            weights[d>=0] = 0\n",
    "\n",
    "            try:\n",
    "                with warnings.catch_warnings(record=True) as caught_warnings:\n",
    "                    warnings.simplefilter(\"always\")\n",
    "                    if np.isinf(dssn):\n",
    "                        dssn = np.finfo(dssn.dtype).max\n",
    "                    exp_term = i * np.abs(d[d<0]) / dssn\n",
    "                    max_exp = 709  # exponential term <= round(np.log(np.finfo(float).max)) to avoid overflow\n",
    "                    exp_term = np.minimum(exp_term, max_exp)\n",
    "                    weights[d<0] = np.exp(exp_term)\n",
    "                    if caught_warnings:\n",
    "                        warning = caught_warnings[0]\n",
    "                        print(f\"Warning at iteration {i} in exp calculation: {warning.message}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Exception at iteration {i} in exp calculation: {e}\")\n",
    "                return z\n",
    "\n",
    "            try:\n",
    "                if np.isinf(dssn):\n",
    "                    dssn = np.finfo(dssn.dtype).max\n",
    "                weights[0] = np.exp(i * (d[d<0]).max() / dssn)\n",
    "            except Warning as warn:\n",
    "                print(f\"Warning at iteration {i} in w[0] calculation: {warn}\")\n",
    "                return z\n",
    "\n",
    "            weights[-1] = weights[0]\n",
    "        return z\n",
    "    except Exception as e:\n",
    "        print(f\"Error in airPLS: {str(e)}\")\n",
    "        print(f\"Traceback: {traceback.format_exc()}\")\n",
    "        return None\n",
    "        \n",
    "def calculate_mae(true, predicted):  \n",
    "    diff = np.abs(true - predicted)\n",
    "    mae = np.mean(diff)    \n",
    "    return mae\n",
    "\n",
    "def calculate_mae_for_params(spectrum, true_spectrum, lambda_, tol):\n",
    "    baseline = airPLS(spectrum, lambda_, porder=2, itermax=1000, tol=tol)\n",
    "    corrected_spectrum = spectrum - baseline\n",
    "    return calculate_mae(true_spectrum, corrected_spectrum)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bedf48d1",
   "metadata": {},
   "source": [
    "### Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2514fe87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from matplotlib.colors import LogNorm\n",
    "\n",
    "def calculate_parameter_space(spectrum, true_spectrum, param_ranges, num_points=50):\n",
    "    lambda_range = np.logspace(np.log10(param_ranges[0, 0]), np.log10(param_ranges[0, 1]), num_points)\n",
    "    tol_range = np.logspace(np.log10(param_ranges[1, 0]), np.log10(param_ranges[1, 1]), num_points)\n",
    "    \n",
    "    mae_values = np.full((num_points, num_points), np.nan)\n",
    "    \n",
    "    for i, lambda_ in enumerate(lambda_range):\n",
    "        for j, tol in enumerate(tol_range):\n",
    "            try:\n",
    "                mae_values[i, j] = calculate_mae_for_params(spectrum, true_spectrum, lambda_, tol)\n",
    "            except Exception as e:\n",
    "                print(f\"Error at lambda={lambda_}, tol={tol}: {str(e)}\")\n",
    "    \n",
    "    valid_mae = mae_values[~np.isnan(mae_values)]\n",
    "    if len(valid_mae) > 0:\n",
    "        optimal_index = np.unravel_index(np.nanargmin(mae_values), mae_values.shape)\n",
    "#         optimal_index = np.unravel_index(np.argmin(mae_values), mae_values.shape)\n",
    "        grid_optimal_lambda = lambda_range[optimal_index[0]]\n",
    "        grid_optimal_tol = tol_range[optimal_index[1]]\n",
    "        grid_optimal_mae = mae_values[optimal_index]\n",
    "    else:\n",
    "        grid_optimal_lambda = grid_optimal_tol = grid_optimal_mae = np.nan\n",
    "    \n",
    "    return {\n",
    "        'lambda_range': lambda_range,\n",
    "        'tol_range': tol_range,\n",
    "        'mae_values': mae_values,\n",
    "        'grid_optimal_lambda': grid_optimal_lambda,\n",
    "        'grid_optimal_tol': grid_optimal_tol,\n",
    "        'grid_optimal_mae': grid_optimal_mae\n",
    "    }\n",
    "\n",
    "def save_calculation_results(results, filename):\n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump(results, f)\n",
    "    print(\"Grid search results saved\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "98f86798",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import lru_cache\n",
    "import time\n",
    "\n",
    "@lru_cache(maxsize=None)\n",
    "def memoized_objective(spectrum, true_spectrum, lambda_, tol):\n",
    "    spectrum_array = np.array(spectrum)\n",
    "    true_spectrum_array = np.array(true_spectrum)\n",
    "    return calculate_mae_for_params(spectrum_array, true_spectrum_array, lambda_, tol)\n",
    "\n",
    "def round_to_6(x):\n",
    "    return np.round(x, 25)\n",
    "\n",
    "def adaptive_grid_search(initial_lambda, initial_tol, initial_resolution, calculate_mae_for_params, spectrum, true_spectrum, param_ranges, min_iterations=5, consecutive_stable_iterations=5, max_non_decreasing=20):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    def objective(params):\n",
    "        try:\n",
    "            return memoized_objective(tuple(spectrum), tuple(true_spectrum), 10**params[0], 10**params[1])\n",
    "        except Exception as e:\n",
    "            print(f\"Error in objective function: {str(e)}\")\n",
    "            print(f\"Parameters: lambda={10**params[0]}, tol={10**params[1]}\")\n",
    "            print(\"Traceback:\")\n",
    "            traceback.print_exc()\n",
    "            return np.inf\n",
    "\n",
    "    center = np.array([np.log10(initial_lambda), np.log10(initial_tol)])\n",
    "    resolution = initial_resolution\n",
    "    num_points = 21\n",
    "\n",
    "    iterations = []\n",
    "    maes = []\n",
    "    lambdas = []\n",
    "    tols = []\n",
    "    relative_errors = []\n",
    "\n",
    "    initial_mae = objective(center)\n",
    "    prev_mae = initial_mae\n",
    "    best_mae = initial_mae\n",
    "    best_params = center.copy()\n",
    "\n",
    "    print(f\"\\nInitial values:\")\n",
    "    print(f\"Lambda: {round_to_6(initial_lambda)}\")\n",
    "    print(f\"Tolerance: {round_to_6(initial_tol)}\")\n",
    "    print(f\"Initial MAE: {round_to_6(initial_mae)}\")\n",
    "\n",
    "    iteration = 0\n",
    "    stable_iterations = 0\n",
    "    non_decreasing_count = 0\n",
    "    \n",
    "    while True:\n",
    "        iteration += 1\n",
    "        print(f\"\\nIteration {iteration}\")\n",
    "        \n",
    "        # Set the best point as the new center\n",
    "        center = best_params.copy()\n",
    "        \n",
    "        # Generate grid in log space, enforcing parameter ranges and including the center point\n",
    "        lambda_range = np.array([10**(center[0] + i*resolution[0]/2) for i in range(-10, 11)])\n",
    "        tol_range = np.array([10**(center[1] + i*resolution[1]/2) for i in range(-10, 11)])\n",
    "        \n",
    "        # Enforce parameter ranges\n",
    "        lambda_range = np.clip(lambda_range, param_ranges[0, 0], param_ranges[0, 1])\n",
    "        tol_range = np.clip(tol_range, param_ranges[1, 0], param_ranges[1, 1])\n",
    "        \n",
    "        print(f\"Lambda range: {round_to_6(lambda_range[0])} to {round_to_6(lambda_range[-1])}\")\n",
    "        print(f\"Tol range: {round_to_6(tol_range[0])} to {round_to_6(tol_range[-1])}\")\n",
    "        print(f\"Resolution: {resolution}\")\n",
    "        \n",
    "        # Calculate MAE for each point in the grid\n",
    "        mae_values = np.zeros((num_points, num_points))\n",
    "        for i, lambda_ in enumerate(lambda_range):\n",
    "            for j, tol in enumerate(tol_range):\n",
    "                mae_values[i, j] = objective([np.log10(lambda_), np.log10(tol)])\n",
    "#                 print(f'MAE value: {mae_values[i, j]}')\n",
    "        \n",
    "#         print(\"MAE values:\")\n",
    "#         print(mae_values)\n",
    "        \n",
    "        # Find the new optimal point, ignoring NaN values\n",
    "        if np.any(~np.isnan(mae_values)):\n",
    "            optimal_index = np.unravel_index(np.nanargmin(mae_values), mae_values.shape)\n",
    "            new_center = np.array([np.log10(lambda_range[optimal_index[0]]), np.log10(tol_range[optimal_index[1]])])\n",
    "            new_mae = mae_values[optimal_index]\n",
    "        else:\n",
    "            print(\"Warning: All MAE values are NaN. Keeping the previous best point.\")\n",
    "            new_center = best_params.copy()\n",
    "            new_mae = best_mae\n",
    "            \n",
    "        print(f\"New center: {round_to_6(10**new_center[0])}, {round_to_6(10**new_center[1])}\")\n",
    "        print(f\"New MAE: {round_to_6(new_mae)}\")\n",
    "        \n",
    "        # Calculate relative error\n",
    "        relative_error = (new_mae - prev_mae) / prev_mae if prev_mae != 0 and not np.isnan(prev_mae) else 0\n",
    "        print(f\"Relative error: {round_to_6(relative_error)}\")\n",
    "        \n",
    "        # Store results (store the best MAE)\n",
    "        iterations.append(iteration)\n",
    "        maes.append(min(new_mae, best_mae))\n",
    "        lambdas.append(10**new_center[0])\n",
    "        tols.append(10**new_center[1])\n",
    "        relative_errors.append(relative_error)\n",
    "        \n",
    "        # Update best MAE and parameters if necessary\n",
    "        if new_mae < best_mae:\n",
    "            best_mae = new_mae\n",
    "            best_params = new_center.copy()\n",
    "            print(\"Best MAE is updated\")\n",
    "            non_decreasing_count = 0\n",
    "        else:\n",
    "            non_decreasing_count += 1\n",
    "        \n",
    "        # Check stopping criteria\n",
    "        if abs(relative_error) < 0.05:\n",
    "            stable_iterations += 1\n",
    "            if stable_iterations >= consecutive_stable_iterations and iteration >= min_iterations:\n",
    "                print(f\"Stopping criterion met: {stable_iterations} consecutive stable iterations\")\n",
    "                break\n",
    "        else:\n",
    "            stable_iterations = 0\n",
    "        \n",
    "        if non_decreasing_count >= max_non_decreasing:\n",
    "            print(f\"Stopping criterion met: {max_non_decreasing} consecutive non-decreasing iterations\")\n",
    "            break\n",
    "        \n",
    "        # Check if we need to adjust resolution\n",
    "        distance_from_center = np.array([\n",
    "            abs(optimal_index[0] - num_points // 2) / (num_points // 2),\n",
    "            abs(optimal_index[1] - num_points // 2) / (num_points // 2)\n",
    "        ])\n",
    "        if np.all(distance_from_center < 0.5):\n",
    "            resolution *= 0.5\n",
    "            print(\"Increasing resolution\")\n",
    "        \n",
    "        # Update for next iteration\n",
    "        prev_mae = new_mae\n",
    "    \n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    \n",
    "    print(f\"\\nOptimization completed in {round_to_6(elapsed_time)} seconds\")\n",
    "    print(f\"Number of iterations: {iteration}\")\n",
    "    print(f\"Best MAE: {round_to_6(best_mae)}\")\n",
    "    print(f\"Best parameters: lambda={round_to_6(10**best_params[0])}, tol={round_to_6(10**best_params[1])}\")\n",
    "    \n",
    "    trajectory = {\n",
    "        'iterations': iterations,\n",
    "        'maes': maes,\n",
    "        'lambdas': lambdas,\n",
    "        'tols': tols,\n",
    "        'relative_errors': relative_errors\n",
    "    }\n",
    "    \n",
    "    return [10**best_params[0], 10**best_params[1]], best_mae, trajectory, memoized_objective.cache_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e6ed3c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_parameter_space_new(calc_results, optimal_params, trajectory, ax, initial_params):\n",
    "    lambda_range = calc_results['lambda_range']\n",
    "    tol_range = calc_results['tol_range']\n",
    "    mae_values = calc_results['mae_values']\n",
    "    \n",
    "    # Create a custom colormap\n",
    "    n_bin = 100\n",
    "    cmap = plt.get_cmap('viridis', n_bin)\n",
    "    \n",
    "    # Use LogNorm for better distribution of colors, emphasizing lower values\n",
    "    norm = LogNorm(vmin=np.nanmin(mae_values), vmax=np.nanmax(mae_values))\n",
    "    \n",
    "    # Create contour plot\n",
    "    contour = ax.pcolormesh(np.log10(lambda_range), np.log10(tol_range), mae_values.T, \n",
    "                            cmap=cmap, norm=norm, shading='auto')\n",
    "    \n",
    "    # Create a custom colorbar\n",
    "    cbar = plt.colorbar(contour, ax=ax, label='MAE (log scale)')\n",
    "    \n",
    "    # Set colorbar ticks to show actual MAE values\n",
    "    tick_locations = np.logspace(np.log10(np.nanmin(mae_values)), np.log10(np.nanmax(mae_values)), num=6)\n",
    "    cbar.set_ticks(tick_locations)\n",
    "    cbar.set_ticklabels([f'{t:.2e}' for t in tick_locations])\n",
    "    \n",
    "    ax.set_xlabel('log10(lambda)')\n",
    "    ax.set_ylabel('log10(tol)')\n",
    "    ax.set_title('MAE Landscape for airPLS Parameters')\n",
    "    \n",
    "    # Plot full optimization trajectory, including initial parameters\n",
    "    full_lambda_trajectory = [initial_params[0]] + trajectory['lambdas']\n",
    "    full_tol_trajectory = [initial_params[1]] + trajectory['tols']\n",
    "    full_iterations = [0] + trajectory['iterations']\n",
    "    \n",
    "    ax.plot(np.log10(full_lambda_trajectory), np.log10(full_tol_trajectory), 'r--', linewidth=2, alpha=0.7)\n",
    "    scatter = ax.scatter(np.log10(full_lambda_trajectory), np.log10(full_tol_trajectory), \n",
    "                         c=full_iterations, cmap='cool', s=50, zorder=5)\n",
    "    \n",
    "    # Add colorbar for iterations\n",
    "    cbar_iter = plt.colorbar(scatter, ax=ax, label='Iteration', pad=0.01)\n",
    "    \n",
    "    # Plot initial parameters\n",
    "    ax.plot(np.log10(initial_params[0]), np.log10(initial_params[1]), 'k*', \n",
    "            markersize=15, label='Initial Parameters')\n",
    "    \n",
    "    # Plot first iteration result\n",
    "    ax.plot(np.log10(trajectory['lambdas'][0]), np.log10(trajectory['tols'][0]), 'g*', \n",
    "            markersize=15, label='First Iteration')\n",
    "    \n",
    "    # Plot final parameters\n",
    "    ax.plot(np.log10(trajectory['lambdas'][-1]), np.log10(trajectory['tols'][-1]), 'w*', \n",
    "            markersize=15, label='Final Parameters')\n",
    "    \n",
    "    optimal_lambda, optimal_tol = optimal_params\n",
    "    ax.plot(np.log10(optimal_lambda), np.log10(optimal_tol), 'y*', \n",
    "            markersize=15, label='Optimal Parameters')\n",
    "    \n",
    "    ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0cba847",
   "metadata": {},
   "source": [
    "### Option 1: process all csv's first n spectra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f79a195b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import traceback\n",
    "\n",
    "def process_all_csvs(base_dir, output_base_dir, start_spectra_index=0, end_spectra_index=4, display_plots=False, single_file=None, single_spectrum_index=None, initial_params=[100, 0.001]):\n",
    "    if single_file and single_spectrum_index is not None:\n",
    "        # Process a single spectrum from a specific file\n",
    "        file_name = os.path.basename(single_file)\n",
    "        type_shape = os.path.splitext(file_name)[0]\n",
    "        output_dir = os.path.join(output_base_dir, type_shape)\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        true_spectra_path = os.path.join(base_dir, \"True spectra\", f\"{type_shape.split('_')[0]}_df.csv\")\n",
    "        true_baseline_path = os.path.join(base_dir, f\"{type_shape}_baseline.csv\")\n",
    "        \n",
    "        param_ranges = np.array([\n",
    "            [1e0, 1e8],  # lambda range\n",
    "            [10**(-7.5), 1e-2]  # tol range\n",
    "        ])\n",
    "        \n",
    "        print(f\"\\nProcessing single spectrum {single_spectrum_index} from {file_name}\")\n",
    "        results, predicted_baselines, predicted_spectra = process_csv_with_optimization_and_grid_search(\n",
    "            single_file, true_spectra_path, true_baseline_path, \n",
    "            param_ranges, initial_params, \n",
    "            output_dir=output_dir, n_spectra=1, display_plots=display_plots,\n",
    "            single_spectrum_index=single_spectrum_index\n",
    "        )\n",
    "        \n",
    "        # Save results for the single spectrum\n",
    "        results.to_csv(os.path.join(output_dir, f\"{type_shape}_spectrum_{single_spectrum_index}_optimization_results.csv\"), index=False)\n",
    "        predicted_baselines.to_csv(os.path.join(output_dir, f\"{type_shape}_spectrum_{single_spectrum_index}_predicted_baseline.csv\"), index=False)\n",
    "        predicted_spectra.to_csv(os.path.join(output_dir, f\"{type_shape}_spectrum_{single_spectrum_index}_predicted_spectra.csv\"), index=False)\n",
    "        \n",
    "    else:\n",
    "        # Process all CSVs or first n_spectra of all CSVs\n",
    "        types = ['broad', 'convoluted', 'distinct']\n",
    "        shapes = ['exp', 'gaussian', 'poly', 'sigmoidal']\n",
    "\n",
    "        param_ranges = np.array([\n",
    "            [1e0, 1e8],  # lambda range\n",
    "            [10**(-7.5), 1e-2]  # tol range\n",
    "        ])\n",
    "\n",
    "        for type in types:\n",
    "            for shape in shapes:\n",
    "                print(f\"\\nProcessing {type}_{shape}\")\n",
    "                \n",
    "                # Define file paths\n",
    "                file_path = os.path.join(base_dir, f\"{type}_{shape}.csv\")\n",
    "                true_spectra_path = os.path.join(base_dir, \"True spectra\", f\"{type}_df.csv\")\n",
    "                true_baseline_path = os.path.join(base_dir, f\"{type}_{shape}_baseline.csv\")\n",
    "                \n",
    "                # Create output directory\n",
    "                output_dir = os.path.join(output_base_dir, f\"{type}_{shape}\")\n",
    "                os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "                # Process the CSV\n",
    "                start_time = time.time()\n",
    "                print(f\"Working on spectral_shape={type}, baseline_shape={shape}\")\n",
    "                results, predicted_baselines, predicted_spectra = process_csv_with_optimization_and_grid_search(\n",
    "                    file_path, true_spectra_path, true_baseline_path, \n",
    "                    param_ranges, initial_params=[100, 0.001], \n",
    "                    output_dir=output_dir, start_spectra_index=start_spectra_index, end_spectra_index=end_spectra_index, display_plots=display_plots\n",
    "                )\n",
    "                end_time = time.time()\n",
    "\n",
    "                # Save results\n",
    "                results.to_csv(os.path.join(output_dir, f\"{type}_{shape}_optimization_results.csv\"), index=False)\n",
    "                predicted_baselines.to_csv(os.path.join(output_dir, f\"{type}_{shape}_predicted_baseline.csv\"), index=False)\n",
    "                predicted_spectra.to_csv(os.path.join(output_dir, f\"{type}_{shape}_predicted_spectra.csv\"), index=False)\n",
    "                \n",
    "                print(f\"Completed {type}_{shape} in {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "    print(\"All processing completed.\")\n",
    "\n",
    "def process_csv_with_optimization_and_grid_search(file_path, true_spectra_path, true_baseline_path, param_ranges, initial_params=[1e4, 1e-3], output_dir=None, start_spectra_index=0, end_spectra_index=4, display_plots=False, single_spectrum_index=None):\n",
    "    print(f\"Processing CSV file: {file_path}\")\n",
    "    print(f\"True spectra file: {true_spectra_path}\")\n",
    "    print(f\"True baseline file: {true_baseline_path}\")\n",
    "    \n",
    "    # Read the CSV files\n",
    "    df = pd.read_csv(file_path)\n",
    "    true_df = pd.read_csv(true_spectra_path, sep='\\t')\n",
    "    true_baseline_df = pd.read_csv(true_baseline_path)\n",
    "    \n",
    "    # Extract spectra\n",
    "    wavenumbers = df.iloc[:, 0]  # Assuming the first column is wavenumber\n",
    "    if single_spectrum_index is not None:\n",
    "        spectra = df.iloc[:, single_spectrum_index:single_spectrum_index+1]\n",
    "    else:\n",
    "        spectra = df.iloc[:, start_spectra_index+1:(end_spectra_index+1)+1]  # Process only the spectra with index from input start to input end\n",
    "    true_spectrum = true_df['y'].values\n",
    "    \n",
    "    print(f\"Total number of spectra to process: {len(spectra.columns)}\")\n",
    "    \n",
    "    results = []\n",
    "    predicted_baselines = pd.DataFrame()\n",
    "    predicted_spectra = pd.DataFrame()\n",
    "    \n",
    "    best_params = initial_params  # Initialize with the provided initial parameters\n",
    "    \n",
    "    for i, col in enumerate(tqdm(spectra.columns, desc=\"Processing spectra\")):\n",
    "        print(f\"\\nProcessing spectrum {i+1}/{len(spectra.columns)}: {col}\")\n",
    "        spectrum = spectra[col].values\n",
    "        true_baseline = true_baseline_df[col].values\n",
    "        \n",
    "        # Use the best parameters from the previous spectrum as the starting point\n",
    "        initial_optimal_params = best_params\n",
    "        \n",
    "        print(f\"Starting optimization with initial parameters: lambda={initial_optimal_params[0]}, tol={initial_optimal_params[1]}\")\n",
    "        \n",
    "        adaptive_grid_result = adaptive_grid_search(\n",
    "            initial_optimal_params[0], initial_optimal_params[1], np.array([0.5, 0.5]),\n",
    "            calculate_mae_for_params, spectrum, true_spectrum,\n",
    "            param_ranges, consecutive_stable_iterations=5, max_non_decreasing=20\n",
    "        )\n",
    "        \n",
    "        # Unpack the result based on the actual number of returned values\n",
    "        if len(adaptive_grid_result) == 4:\n",
    "            optimal_params, optimal_mae, trajectory, cache_info = adaptive_grid_result\n",
    "        elif len(adaptive_grid_result) == 3:\n",
    "            optimal_params, optimal_mae, trajectory = adaptive_grid_result\n",
    "            cache_info = None\n",
    "        else:\n",
    "            raise ValueError(f\"Unexpected number of return values from adaptive_grid_search: {len(adaptive_grid_result)}\")\n",
    "        \n",
    "        # Update best_params for the next iteration\n",
    "        best_params = optimal_params\n",
    "        \n",
    "        # Apply airPLS with final optimal parameters\n",
    "        print(\"Applying airPLS with final optimal parameters...\")\n",
    "        baseline = airPLS(spectrum, lambda_=optimal_params[0], porder=2, itermax=1000, tol=optimal_params[1])\n",
    "        \n",
    "        # Calculate MAEs\n",
    "        baseline_mae = calculate_mae(true_baseline, baseline)\n",
    "        spectrum_mae = calculate_mae(true_spectrum, spectrum - baseline)\n",
    "        \n",
    "        print(f\"Results for spectrum {col}:\")\n",
    "        print(f\"  Optimal lambda: {optimal_params[0]}\")\n",
    "        print(f\"  Optimal tolerance: {optimal_params[1]}\")\n",
    "        print(f\"  Spectrum MAE: {spectrum_mae}\")\n",
    "        \n",
    "        results.append({\n",
    "            'spectrum': col,\n",
    "            'optimal_lambda': optimal_params[0],\n",
    "            'optimal_tol': optimal_params[1],\n",
    "            'baseline_mae': baseline_mae,\n",
    "            'spectrum_mae': spectrum_mae\n",
    "        })\n",
    "        \n",
    "        # Store predicted baseline and spectrum\n",
    "        predicted_baselines[col] = baseline\n",
    "        predicted_spectra[col] = spectrum - baseline\n",
    "        \n",
    "        # Plot and save the results\n",
    "        if output_dir:\n",
    "            try:\n",
    "                print(f\"Attempting to save plots for spectrum {col}\")\n",
    "                \n",
    "                # Ensure output directory exists\n",
    "                os.makedirs(output_dir, exist_ok=True)\n",
    "                print(f\"Output directory confirmed: {output_dir}\")\n",
    "\n",
    "                # Spectrum plot\n",
    "                plt.figure(figsize=(12, 8))\n",
    "                plt.plot(wavenumbers, spectrum, label='Original Spectrum')\n",
    "                plt.plot(wavenumbers, true_baseline, label='True Baseline')\n",
    "                plt.plot(wavenumbers, baseline, label='Estimated Baseline')\n",
    "                plt.plot(wavenumbers, spectrum - baseline, label='Corrected Spectrum')\n",
    "                plt.plot(wavenumbers, true_spectrum, label='True Spectrum')\n",
    "                plt.xlabel('Wavenumber')\n",
    "                plt.ylabel('Intensity')\n",
    "                plt.title(f'Spectrum {col} - AirPLS Baseline Correction\\n' +\n",
    "                          f'Optimal λ: {optimal_params[0]:.2e}, tol: {optimal_params[1]:.2e}, MAE: {spectrum_mae:.4f}')\n",
    "                plt.legend()\n",
    "                plt.tight_layout()\n",
    "                spectrum_plot_path = os.path.join(output_dir, f'spectrum_{col}_plot.png')\n",
    "                plt.savefig(spectrum_plot_path)\n",
    "                print(f\"Spectrum plot saved: {spectrum_plot_path}\")\n",
    "                plt.close('all')\n",
    "                plt.clf()\n",
    "                plt.cla()\n",
    "                \n",
    "                # Trajectory plots (2x3)\n",
    "                fig, axs = plt.subplots(3, 2, figsize=(15, 20))\n",
    "                \n",
    "                axs[0, 0].plot(trajectory['iterations'], trajectory['maes'])\n",
    "                axs[0, 0].set_xlabel('Iteration')\n",
    "                axs[0, 0].set_ylabel('MAE')\n",
    "                axs[0, 0].set_title('MAE vs Iteration')\n",
    "                \n",
    "                axs[0, 1].plot(trajectory['iterations'], np.log10(trajectory['maes']))\n",
    "                axs[0, 1].set_xlabel('Iteration')\n",
    "                axs[0, 1].set_ylabel('log10(MAE)')\n",
    "                axs[0, 1].set_title('log10(MAE) vs Iteration')\n",
    "                \n",
    "                axs[1, 0].plot(trajectory['iterations'], np.log10(trajectory['lambdas']))\n",
    "                axs[1, 0].set_xlabel('Iteration')\n",
    "                axs[1, 0].set_ylabel('log10(lambda)')\n",
    "                axs[1, 0].set_title('log10(lambda) vs Iteration')\n",
    "                \n",
    "                axs[1, 1].plot(trajectory['iterations'], np.log10(trajectory['tols']))\n",
    "                axs[1, 1].set_xlabel('Iteration')\n",
    "                axs[1, 1].set_ylabel('log10(tol)')\n",
    "                axs[1, 1].set_title('log10(tol) vs Iteration')\n",
    "                \n",
    "                axs[2, 0].plot(trajectory['iterations'], trajectory['relative_errors'])\n",
    "                axs[2, 0].set_xlabel('Iteration')\n",
    "                axs[2, 0].set_ylabel('Relative Error')\n",
    "                axs[2, 0].set_title('Relative Error vs Iteration')\n",
    "                \n",
    "                axs[2, 1].axis('off')\n",
    "                \n",
    "                for ax in axs.flat:\n",
    "                    if ax.get_title():  # Only add grid to non-empty plots\n",
    "                        ax.grid(True)\n",
    "                \n",
    "                plt.tight_layout()\n",
    "                trajectory_plot_path = os.path.join(output_dir, f'spectrum_{col}_trajectory_plots.png')\n",
    "                plt.savefig(trajectory_plot_path)\n",
    "                print(f\"Trajectory plots saved: {trajectory_plot_path}\")\n",
    "                plt.close('all')\n",
    "                plt.clf()\n",
    "                plt.cla()\n",
    "                \n",
    "                print(f\"All plots for spectrum {col} saved successfully\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error occurred while generating plots for spectrum {col}:\")\n",
    "                print(traceback.format_exc())\n",
    "            \n",
    "        print(f\"Completed processing spectrum {col}\")\n",
    "        print(memoized_objective.cache_info())\n",
    "        memoized_objective.cache_clear()\n",
    "    \n",
    "    # Add wavenumbers to predicted baselines and spectra\n",
    "    predicted_baselines['wavenumbers'] = wavenumbers\n",
    "    predicted_spectra['wavenumbers'] = wavenumbers\n",
    "    \n",
    "    # Convert results to DataFrame for easy analysis\n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    print(\"\\nOptimization Results Summary:\")\n",
    "    print(results_df.describe())\n",
    "    \n",
    "    return results_df, predicted_baselines, predicted_spectra"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d1ad2f",
   "metadata": {},
   "source": [
    "#### Option 1:\n",
    "\n",
    "To process the i-th to j-th spectra of all CSVs:\n",
    "\n",
    "Example: start_spectra_index=0: we start from the the first spectrum, end_spectra_index=4:  we stop after processing the 5th spectrum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f64890cd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Main execution\n",
    "base_dir = r\"E:\\python\\Jupyter\\PhD\\Raman Spectrum\\Baseline removal\\New_simulated\\data\\one spectral shape\"\n",
    "output_base_dir = r\"E:\\python\\Jupyter\\PhD\\Raman Spectrum\\Baseline removal\\New_simulated\\results\\airpls_optimization\"\n",
    "\n",
    "print(\"Starting AirPLS optimization pipeline...\")\n",
    "process_all_csvs(base_dir, output_base_dir, start_spectra_index=0, end_spectra_index=4, display_plots=False) \n",
    "print(\"\\nAll processing completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275b4ee2",
   "metadata": {},
   "source": [
    "#### Option 2:\n",
    "\n",
    "To process a single spectrum from one CSV:\n",
    "\n",
    "Example: broad_gaussian.csv: the csv we are going to process, single_spectrum_index=4: to process the 4th spectrum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59560d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main execution\n",
    "base_dir = r\"E:\\python\\Jupyter\\PhD\\Raman Spectrum\\Baseline removal\\New_simulated\\data\\one spectral shape\"\n",
    "output_base_dir = r\"E:\\python\\Jupyter\\PhD\\Raman Spectrum\\Baseline removal\\New_simulated\\results\\airpls_optimization\"\n",
    "\n",
    "print(\"Starting AirPLS optimization pipeline...\")\n",
    "\n",
    "# OR, to process a single spectrum (e.g., spectrum #3, then single_spectrum_index=4) from a specific CSV (e.g., broad_gaussian.csv):\n",
    "single_file = r\"E:\\python\\Jupyter\\PhD\\Raman Spectrum\\Baseline removal\\New_simulated\\data\\one spectral shape\\broad_gaussian.csv\"\n",
    "process_all_csvs(base_dir, output_base_dir, single_file=single_file, single_spectrum_index=4, initial_params=[100, 0.001])\n",
    "\n",
    "print(\"\\nAll processing completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec50f83e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
